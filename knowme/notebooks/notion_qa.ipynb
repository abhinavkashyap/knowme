{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from knowme.ingest import NotionIngestor\n",
    "from knowme.embedder import ChromaEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestor = NotionIngestor(\n",
    "    folder_path=\"/Users/abhinavkashyap/abhi/projects/knowme/abhinav_notion\"\n",
    ")\n",
    "texts, metadatas = ingestor.ingest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = text_splitter.create_documents(texts=texts, metadatas=metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Let's explore this paper to better understand the claimed advantages and improvements over GPT-3.\\n\\n## Mixture of Experts\\n\\n![Screenshot 2023-11-27 at 5.38.08 PM.png](GLaM%20Efficient%20Scaling%20of%20Language%20Models%20with%20Mix%20daa1c17d134249129c43a311d3ff31c5/Screenshot_2023-11-27_at_5.38.08_PM.png)\\n\\nThis is a transformer block. The major computational bottleneck in Transformers is the FFN or the Feedforward Neural Network Component. Instead of a single FFN layer, can we replace it with multiple FFN modules? We can. These individual FFN modules are called experts. Doing this can have multiple advantages:\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(documents))\n",
    "documents[10].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "store = ChromaEmbedder(\n",
    "    text_splitter,\n",
    "    embedding_function=embedding_function,\n",
    "    embedding_store_directory=\"./website_vectorstore\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromastore = store.store_embeddings(texts, metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = chromastore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "retrieved_docs = retriever.invoke(\"What does Abhinav work as?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[https://notaku.so/iframes/clickable-image/eyJpbWFnZVVybCI6Imh0dHBzOi8vZW5jcnlwdGVkLXRibjAuZ3N0YXRpYy5jb20vaW1hZ2VzP3E9dGJuOkFOZDlHY1RMV0FlbWMzMFZRZkluMEFuU3AybDE1dUR2WXptYUZfdU1YZyZ1c3FwPUNBVSIsImxpbmtIcmVmIjoiaHR0cHM6Ly90d2l0dGVyLmNvbS9hYmthc2h5YXA5MiJ9](https://notaku.so/iframes/clickable-image/eyJpbWFnZVVybCI6Imh0dHBzOi8vZW5jcnlwdGVkLXRibjAuZ3N0YXRpYy5jb20vaW1hZ2VzP3E9dGJuOkFOZDlHY1RMV0FlbWMzMFZRZkluMEFuU3AybDE1dUR2WXptYUZfdU1YZyZ1c3FwPUNBVSIsImxpbmtIcmVmIjoiaHR0cHM6Ly90d2l0dGVyLmNvbS9hYmthc2h5YXA5MiJ9)\\n\\nHey Iâ€™m Abhinav. I am a AI Scientist at ASUS-AICS. I work on Natural Language Processing and I am  interested in processing clinical notes.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some of the information is there in the first retrieved document\n",
    "# I might have to do some cleaning on the ingestion side\n",
    "# Remove the links in the page that contain embeddable content\n",
    "# Consider only those that only contain text.\n",
    "# This might require a Markdown parser to filter content out\n",
    "retrieved_docs[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knowme.knowme_chat import KnowmeChat\n",
    "\n",
    "chat = KnowmeChat(llm, chromastore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chat.chat(\"Where is Abhinav working?\", session_id=\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abhinav is currently working as an AI Scientist at ASUS-AICS in Singapore. His work focuses on Natural Language Processing, particularly processing clinical notes.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During his PhD, Abhinav worked with Professor Min-Yen Kan. They collaborated on understanding text from various sources, not just mainstream media.\n"
     ]
    }
   ],
   "source": [
    "answer = chat.chat(\"Who did Abhinav work with during PhD?\", session_id=\"abc\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chat.chat(\"What is the news from Abhinav from March 2024\", session_id=\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In March 2024, Abhinav presented a survey paper on sentence representations at the European Chapter of the Association for Computational Linguistics (EACL).'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, there is no other news mentioned for Abhinav in March 2024. The only event listed is his presentation of a survey paper on sentence representations at EACL.\n"
     ]
    }
   ],
   "source": [
    "print(chat.chat(\"Is there any other news from him for March 2024\", session_id=\"abc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In his blog post on GLaM, Abhinav mentions three key points:\n",
      "\n",
      "1. GLaM is the first model to incorporate a mixture of experts into the decoder model, which allows it to perform better than GPT-3 with half the number of parameters for inference. \n",
      "2. The GLaM paper introduces a new dataset, which contains 1.6 trillion tokens from the web, to increase the diversity of the data.\n",
      "3. There are challenges in training mixture of models, such as deciding which expert handles which tokens (also known as routing), and determining the number of experts to add. These decisions often require experimentation.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    chat.chat(\n",
    "        \"In his blog post on GLaM, what are the three things he mentions?\",\n",
    "        session_id=\"abc\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The titles of the articles written by Abhinav as mentioned in the context are:\n",
      "\n",
      "1. \"GLaM: Efficient Scaling of Language Models with Mixture of Experts\"\n",
      "2. \"Publicly Shareable Clinical Large Language model Built on Synthetic Clinical Notes\"\n",
      "3. \"MAUVE - Measuring the Gap Between Neural text and Human Text using Divergence Frontiers\"\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    chat.chat(\n",
    "        \"What are the titles of the different articles that Abhinav has written?\",\n",
    "        session_id=\"abc\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "store_from_directory = Chroma(\n",
    "    persist_directory=\"./website_vector_store\", embedding_function=embedding_function\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowme-Vm6gcGsG-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
