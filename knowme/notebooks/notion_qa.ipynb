{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from knowme.ingest import NotionIngestor\n",
    "from knowme.embedder import ChromaEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestor = NotionIngestor(\n",
    "    folder_path=\"/Users/abhinavkashyap/abhi/projects/knowme/abhinav_notion\"\n",
    ")\n",
    "texts, metadatas = ingestor.ingest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Abhinav Ramesh Kashyap\\n\\n![abhinav_pic.jpeg](Abhinav%20Ramesh%20Kashyap%202a5b5c4c70f240359dc297eaa025b430/abhinav_pic.jpeg)\\n\\n[https://notaku.so/iframes/clickable-image/eyJpbWFnZVVybCI6Imh0dHBzOi8vZW5jcnlwdGVkLXRibjAuZ3N0YXRpYy5jb20vaW1hZ2VzP3E9dGJuOkFOZDlHY1FPWnRzS0VJMTFOR3FMVk1ObFZ2WE5fUTZUeVhNUXVPZ2Q1U0IwV0Y2TzdRJnMiLCJsaW5rSHJlZiI6Imh0dHBzOi8vc2Nob2xhci5nb29nbGUuY29tL2NpdGF0aW9ucz91c2VyPXVjM3UwWkFBQUFBSiZobD1lbiJ9](https://notaku.so/iframes/clickable-image/eyJpbWFnZVVybCI6Imh0dHBzOi8vZW5jcnlwdGVkLXRibjAuZ3N0YXRpYy5jb20vaW1hZ2VzP3E9dGJuOkFOZDlHY1FPWnRzS0VJMTFOR3FMVk1ObFZ2WE5fUTZUeVhNUXVPZ2Q1U0IwV0Y2TzdRJnMiLCJsaW5rSHJlZiI6Imh0dHBzOi8vc2Nob2xhci5nb29nbGUuY29tL2NpdGF0aW9ucz91c2VyPXVjM3UwWkFBQUFBSiZobD1lbiJ9)\\n\\n[https://notaku.so/iframes/clickable-image/eyJpbWFnZVVybCI6Imh0dHBzOi8vZW5jcnlwdGVkLXRibjAuZ3N0YXRpYy5jb20vaW1hZ2VzP3E9dGJuOkFOZDlHY1RBRDN2N2hqS1BUV2hZQ1loXzhnRWU2ekJUVGJzdmFTdFZPWERVcnpTNGRnJnMiLCJsaW5rSHJlZiI6Imh0dHBzOi8vd3d3LnNlbWFudGljc2Nob2xhci5vcmcvbWUvcmVzZWFyY2gifQ==](https://notaku.so/iframes/clickable-image/eyJpbWFnZVVybCI6Imh0dHBzOi8vZW5jcnlwdGVkLXRibjAuZ3N0YXRpYy5jb20vaW1hZ2VzP3E9dGJuOkFOZDlHY1RBRDN2N2hqS1BUV2hZQ1loXzhnRWU2ekJUVGJzdmFTdFZPWERVcnpTNGRnJnMiLCJsaW5rSHJlZiI6Imh0dHBzOi8vd3d3LnNlbWFudGljc2Nob2xhci5vcmcvbWUvcmVzZWFyY2gifQ==)\\n\\n[https://notaku.so/iframes/clickable-image/eyJpbWFnZVVybCI6Imh0dHBzOi8vZW5jcnlwdGVkLXRibjAuZ3N0YXRpYy5jb20vaW1hZ2VzP3E9dGJuOkFOZDlHY1RMV0FlbWMzMFZRZkluMEFuU3AybDE1dUR2WXptYUZfdU1YZyZ1c3FwPUNBVSIsImxpbmtIcmVmIjoiaHR0cHM6Ly90d2l0dGVyLmNvbS9hYmthc2h5YXA5MiJ9](https://notaku.so/iframes/clickable-image/eyJpbWFnZVVybCI6Imh0dHBzOi8vZW5jcnlwdGVkLXRibjAuZ3N0YXRpYy5jb20vaW1hZ2VzP3E9dGJuOkFOZDlHY1RMV0FlbWMzMFZRZkluMEFuU3AybDE1dUR2WXptYUZfdU1YZyZ1c3FwPUNBVSIsImxpbmtIcmVmIjoiaHR0cHM6Ly90d2l0dGVyLmNvbS9hYmthc2h5YXA5MiJ9)\\n\\nHey I‚Äôm Abhinav. I am a AI Scientist at ASUS-AICS. I work on Natural Language Processing and I am  interested in processing clinical notes. \\n\\n‚ÄúLet me try my hand at machine learning‚Äù started my exploration on the subject during my Masters. I stayed in college for my PhD.  I was advised by [Prof Min-Yen Kan](https://www.comp.nus.edu.sg/~kanmy/)  and we worked on understanding text originating from different sources and not just mainstream media.  I now focus on help machines understand clinical notes and other text written by clinicians and doctors as a scientist in Singapore. \\n\\nI started as a web developer after my graduation in Bangalore. Developing websites and web in general still excites me. Apart from training machines to learn about human languages, I enjoy playing my guitar and singing. I also  [sketch](https://www.instagram.com/art.abhi.asa/) in my spare time. \\n\\n**OPEN TO NEW OPPORTUNITIES -** I am looking for a Machine Learning Engineer or  an Applied Science role. I am willing to relocate to India. Contact me. \\n\\n<aside>\\nüì∞ **NEWS**\\n\\n</aside>\\n\\n- ***May 2024***- Our Paper M-QALM a benchmark to test the abilities of clinical large l anguage models is accepted to ACL‚Äô24 Findings\\n- ***March 2024***- Presented our survey paper on sentence representations in EACL\\n- ***January 2024***- Our survey paper on sentence representations is accepted at EACL‚Äô24\\n- ***November 2023*** - Invited Talk at OntarioTechU/University of Toronto.\\n- ***November 2023** -* Invited Guest Lecture at SUTD on LLMs and Clinical NLP\\n- ***July 2023*** - We stand second in the ImagCLEF competition \\nto summarize clinical dialogues. [Paper](https://arxiv.org/abs/2307.02006)\\n- ***June 2023 -*** We stand second in a competition to generate problem lists from clinical notes.  [Paper](https://aclanthology.org/2023.bionlp-1.49/)\\n- ***April 2023 -*** Our paper that predicts ICD codes from clinical notes using a clever hierarchical decomposition is accepted to ACL 2023. Paper [here](https://arxiv.org/abs/2306.00005).\\n- ***April 2023 -*** We intend to make speech synthesis work for different kinds of speakers quickly. We will present our paper Adapter Mix in INTERSPEECH‚Äô23. Paper here\\n\\n<aside>\\nüßë\\u200düî¨ **PAPER NOTES**\\nI write notes about the papers I read and post it here.\\n\\n</aside>\\n\\n[GLaM: Efficient Scaling of Language Models with Mixture of Experts ](Abhinav%20Ramesh%20Kashyap%202a5b5c4c70f240359dc297eaa025b430/GLaM%20Efficient%20Scaling%20of%20Language%20Models%20with%20Mix%20daa1c17d134249129c43a311d3ff31c5.md)\\n\\n[Publicly Shareable Clinical Large Language model Built on Synthetic Clinical Notes](Abhinav%20Ramesh%20Kashyap%202a5b5c4c70f240359dc297eaa025b430/Publicly%20Shareable%20Clinical%20Large%20Language%20model%20B%20f11a9464b4f44362a1b28ef44c9f32ea.md)\\n\\n[MAUVE - Measuring the Gap Between Neural text and Human Text using Divergence Frontiers](Abhinav%20Ramesh%20Kashyap%202a5b5c4c70f240359dc297eaa025b430/MAUVE%20-%20Measuring%20the%20Gap%20Between%20Neural%20text%20and%20%208e061734893e41fcb7cb8e1bc42b4c0b.md)\\n\\n[Understanding Dataset Difficulty](Abhinav%20Ramesh%20Kashyap%202a5b5c4c70f240359dc297eaa025b430/Understanding%20Dataset%20Difficulty%20ad876cd2fd84411db2175ce749005524.md)\\n\\n<aside>\\nüèôÔ∏è **OTHER WRITINGS** \\nOther writings. General Musings\\n\\n</aside>\\n\\n---\\n\\nThanks to [Reza Arkan](https://rezaarkan.com/) for inspiring me to create my website using Notion. I made this website using Notion And [Fruition](https://www.notion.so/Fruition-Free-Open-Source-Toolkit-for-Building-Websites-with-Notion-771ef38657244c27b9389734a9cbff44?pvs=21). Give it a try.',\n",
       " \"# GLaM: Efficient Scaling of Language Models with Mixture of Experts\\n\\nLanguage models are continually increasing in size, following the belief that larger models are inherently better‚Äîa principle known as the scaling laws of large language models. Scaling typically involves adding more layers and parameters, demanding significant computational resources for training. This process presents challenges, making the training of such large language models complex.\\n\\nHowever, the Mixture of Experts offers an alternative that addresses some scaling issues. There's speculation that models like ChatGPT and GPT-4 employ this approach. While I can't confirm this, a paper suggests that these models surpass GPT-3 in various cases.\\n\\nLet's explore this paper to better understand the claimed advantages and improvements over GPT-3.\\n\\n## Mixture of Experts\\n\\n![Screenshot 2023-11-27 at 5.38.08 PM.png](GLaM%20Efficient%20Scaling%20of%20Language%20Models%20with%20Mix%20daa1c17d134249129c43a311d3ff31c5/Screenshot_2023-11-27_at_5.38.08_PM.png)\\n\\nThis is a transformer block. The major computational bottleneck in Transformers is the FFN or the Feedforward Neural Network Component. Instead of a single FFN layer, can we replace it with multiple FFN modules? We can. These individual FFN modules are called experts. Doing this can have multiple advantages: \\n\\n1. ***Experts can specialise in certain kinds of inputs:*** depending on the kind of input, we can chose the expert that is the most suitable to handle the input. For example, one expert might handle English and the other may handle Hindi or one expert may handle one kind of word and the other may handle different kind of input \\n2. ***Scaling the model size without really scaling the compute-*** Adding more experts per layer increases the size of the model. However, we can chose only a few experts per input for processing. This keeps the computational requirement still low\\n\\nThere are many challenges in training mixture of models \\n\\n1. ***How do you decide which expert handles which tokens? -*** This is something called as routing. If this sis not done properly \\n2. ***How many experts to add? -*** In some cases it is clear. For example, if you have 3 languages, you need 3 experts per language. However, in most cases it is not very clear how on how many experts to add to the model. It is left to experimentation in most cases. \\n\\nThe GLaM paper is the first model that adds mixture of experts to the decoder model. With half the number of parameters for inference, they perform better than GPT-3. Here is their recipe success. \\n\\n***Model***  - *Use the mixture of experts* model that is shown above.  They are the first to test it on a decoder only language model ‚Äî with the next word prediction objective. They add Mixture of Experts in every other layer of the transformer. The number of experts ranges between 32 and 256 per layer. When they route the tokens, they only select the top-2 experts. \\n\\n***Dataset -*** The paper collects a *new dataset that has 1.6 Trillion tokens* from the web. They follow the same process as GPT3 in preprocessing the dataset. They also include social conversations from social media to increase the diversity of the data. \\n\\n***Evaluation -*** Similar to other papers, they test the zero shot and few-shot performance on *Natural Language Generation and Few-shot Generation* datasets. These are either QA datasets that require generation or those that can chose from multiple options\\n\\nLooking at the results from the paper, I will leave you with three things that I learned from the paper. \\n\\n### Three Things that I learned from the paper\\n\\n***Dense Models use too much computation -*** MoE models achieve better results using only 50% of parameters compared to GPT-3\\n\\n***MoE Models scale*** - When x number of experts are added to a bigger model, they perform better than adding the same x number of experts in the smaller model \\n\\n***Better convergence*** - The model obtains better results compared to dense model when they are trained on same number of tokens\\n\\nThese models are private. They are not open source. Even GPT-4 has been purported to be. Keep an eye for these kinds of models\",\n",
       " '# Publicly Shareable Clinical Large Language model Built on Synthetic Clinical Notes\\n\\nClinical language models hold tremendous promise in enhancing the effectiveness of healthcare professionals. Recently, there has been a significant surge of interest in the development of clinical language models. One recent efforts in this direction include the creation of [Clinical T5](https://huggingface.co/xyla/Clinical-T5-Large). \\n\\nHowever, a notable challenge in developing large-scale clinical language models lies in the use of data. Clinical NLP relies primarily on  MIMIC-III, which comprises clinical notes from a private hospital. This data can only be utilized for non-commercial purposes due to its private and sensitive nature.\\n\\nThere have been numerous calls for a collaborative approach to clinical model development through data sharing. Nonetheless, such collaborative efforts are typically long-term endeavours that require considerable time and coordination.\\n\\nAlternatively, we can leverage the capabilities of large language models, which excel at generating text. One potential strategy is to use these models to generate synthetic clinical notes and then train large language models on this synthetic data. The hope is that these models will perform comparably to those trained on real clinical notes\\n\\n# Generating Synthetic Notes\\n\\nThe authors of this paper have made an astute observation. They\\'ve noted that, unlike clinical notes, case reports are not subject to privacy constraints. In the context of inpatient healthcare, many clinical language models are trained on a specific type of document known as discharge summaries. These summaries encompass a patient\\'s entire stay and are structured, typically including details about allergies, conducted tests, post-discharge instructions, and more.\\n\\nHowever, when it comes to case reports, they lack this structured information. This is where GPT comes into play. The authors use GPT 3.5 to generate synthetic clinical notes that resemble original notes.\\n\\n<aside>\\nü§î ***What strategies can be employed to ensure that synthetic notes closely resemble authentic clinical records?*** \\nOne method involves conducting preliminary testing on a subset of these synthetic notes before deploying GPT-3.5 for large-scale generation. Additionally, an alternative approach is to train a language model using actual clinical notes and subsequently evaluate its perplexity on the synthetic notes. This evaluation serves as an indicator of the fidelity of the generated clinical notes.\\n\\nAlso they could have used something like MAUVE (Read [here](https://abhinavkashyap.bio/MAUVE-Measuring-the-Gap-Between-Neural-text-and-Human-Text-using-Divergence-Frontiers-8e061734893e41fcb7cb8e1bc42b4c0b)) to compare human generated text with machine generated text\\n\\n</aside>\\n\\n## Using Clinical Notes to Generate Instruction Based Data\\n\\nInstruction data comprises both an instruction and its corresponding answer. The training of instruction-based language models has demonstrated recent success. In this paper, clinical notes generated in a prior step are employed to construct an instruction dataset. The study focuses on various tasks deemed relevant in clinical settings, such as Named Entity Recognition, paraphrasing, and coreference resolution, among others.\\n\\nEarly endeavors to assemble instruction-based data involved human intervention. However, recent initiatives like Self-Instruct have shown that Large Language models can autonomously generate instructions and their corresponding answers. In line with these advancements, this paper follows a similar approach by generating instructions for the tasks using Self-Instruct. This process involves the following steps:\\n\\n1. Experts provide initial seed instructions for each task type.\\n2. These seed instructions are used to prompt ChatGPT, which generates additional instructions.\\n3. The generated instruction is then paired with the clinical note and input into ChatGPT once more to obtain the answer\\n\\nThats it. Train LLAMA or any other model in this instruction data to get a task-specific instruction finetuned large language model and evaluate it. Performance of model trained on synthetic notes $\\\\sim$ model trained on real notes is the amin highlight of the paper. However, they use GPT-4 for evaluation. \\n\\n<aside>\\nü§î ***GPT-4 as the only evaluation is problematic***\\nThis paper appears to exclusively rely on GPT-4 for evaluation, which raises some concerns. For instance, it could benefit from employing additional automated evaluation metrics, such as ROUGE, to assess the quality of summarization. While the use of GPT-4 offers the advantage of consolidating all tasks under a single scoring system, the question of whether this alone suffices in this context remains a point of doubt\\n\\n</aside>\\n\\nThe paper leaves several questions unanswered:\\n\\n1. What quantity of synthetic notes is required to attain the performance level of a model trained on authentic notes?\\n2. Does the performance continue to improve as the number of synthetic notes is increased?\\n3. How diverse is the dataset generated by the language model, and does this diversity impact the model\\'s performance?\"\\n\\nI kind of enjoyed reading the paper.  Clinical language models require [large scale data](https://www.statnews.com/2023/08/25/ai-medicine-large-language-models/?utm_campaign=twitter_organic&utm_source=twitter&utm_medium=social). Combining data from multiple hospitals is often prohibited to develop language models. This paper addresses such concerns and is a first step in enabling such large language models for clinical usecases.\\n\\nIf you would like to discuss the paper or exchange thoughts further, feel free to reach out via email at [**abkashyap92@protonmail.com**](mailto:abkashyap92@protonmail.com).',\n",
       " \"# MAUVE - Measuring the Gap Between Neural text and Human Text using Divergence Frontiers\\n\\nMachines can now produce coherent text, with some being virtually indistinguishable from human-authored content. This development carries significant implications, such as enabling students to potentially cheat on their take-home essay exams. Conversely, the overarching aim of machine learning models is to generate text that exhibits maximum human-like qualities. In both of these scenarios, the pressing question is: How can we effectively measure the disparity between machine-generated text and human-generated text? This paper endeavors to propose metrics for precisely evaluating this distinction.\\n\\n## The Tale of 2 Kinds of Errors\\n\\n1. **Type I Error** - In this case, machines produce text that lacks the natural touch of human authorship, making it evident that it wasn't written by a human.\\n2. **Type II Error** - Here, machines fail to generate text that aligns with the typical style and content that a human would naturally produce.\\n\\nBy distinguishing and measuring these two categories of errors, we can effectively quantify the extent to which machine-generated text differs from human-generated text. MAUVE proposes to use an information theoretic measure to do this. \\n\\n## KL Divergence\\n\\nLet‚Äôs say that the distribution of human generated text is captured by $P$ and the distribution of machine generated text is captured by $Q$. \\n\\nType I Error can be calculated using KL Divergence as follows.  Notice that if $Q(x)$  is huge and $P(x)$  is small, then this quantity will be high. That is, the machine generates text, that wouldnt have been really produced by humans. \\n\\n$$\\nKL(Q|P) = \\\\sum\\\\limits_{x} Q(x) log\\\\ \\\\frac{Q(x)}{P(x)}\\n$$\\n\\nSimilarly, Type II errors can be calculated as follows \\n\\n$$\\nKL(P|Q) = \\\\sum\\\\limits_{x} P(x) log\\\\ \\\\frac{P(x)}{Q(x)}\\n$$\\n\\nIn practical applications, these two types of errors can arise from various factors. For instance, machine learning algorithms often have limitations on the number of words they can generate, leading to arbitrary truncation of text. While such truncation may be feasible under distribution $Q$, it wouldn't occur under distribution $P$. To encompass a broad spectrum of these scenarios, this paper suggests employing a distinct reference distribution. \\n\\n$$\\nR_{\\\\lambda}= \\\\lambda KL(Q |P) + (1 - \\\\lambda) KL(P|Q)\\n$$\\n\\nDifferent values of $\\\\lambda$ captures different reference distributions and we can collect many such reference distributions. Then we can plot a divergence curve using the following coordinates. Here $c$ is a constant.  The area under such a curve is the MAUVE score. \\n\\n$$\\nC(P, Q) = (e^{-cKL(Q, R_{\\\\lambda)}}, e^{-cKL(P, R_\\\\lambda)})\\n$$\\n\\n<aside>\\nü§î This curve is similar to the False Positive Rate vs True Positive Rate curve that is famous in machine learning. The area under such a curve is called the AUC-ROC ‚Äî the area under the curve receiving operating characteristic. The authors use a similar intuition here to calculate MAUVE.\\n\\n</aside>\\n\\n## Okay What Should I do in Practice?\\n\\n1. **Convert Human-Written Text to Probability Distribution Q**:\\n    - Use a language model to obtain representations of each human-written text.\\n    - Apply a clustering algorithm like K-Means to group these representations into clusters. Each text is assigned to a cluster.\\n    - Calculate the distribution of data points within each cluster, representing the number of texts assigned to each cluster.\\n2. **Convert Machine-Generated Text to Probability Distribution K**:\\n    - Similarly, obtain representations of each machine-generated text using the same language model.\\n    - Use K-Means or a similar clustering algorithm to cluster the machine-generated text representations into clusters.\\n    - Calculate the distribution of data points within each machine-generated text cluster.\\n3. **Calculate Divergence Curve and MAUVE Score**:\\n    - Utilize the divergence measure, such as the C(P, Q) measure, to compute the divergence between the two probability distributions (P and Q).\\n    - Plot the divergence curve based on these calculations.\\n    - Finally, calculate the area under the divergence curve. This can be done easily using your preferred machine learning package with a simple line of code.\\n    \\n\\n## How do we know that this measure is good?\\n\\nIndeed, the MAUVE score can be a valuable metric for assessing machine-generated text quality, and it often exhibits several favorable characteristics:\\n\\n1. **Correlation with Human Judgment**: MAUVE scores tend to show a positive correlation with human judgments of text quality. In other words, when MAUVE scores are higher, indicating text that is more human-like, it often aligns with human evaluators' perceptions of the quality of the text. This suggests that MAUVE is a useful proxy for human judgment in assessing text authenticity.\\n2. **Agreement with General Trends**: MAUVE scores align with broader trends observed in text generation. For instance, larger and more sophisticated language models are known to produce more coherent and contextually accurate text. MAUVE reflects this trend by assigning higher scores to text generated by such models. This indicates that MAUVE is sensitive to the capabilities of the underlying text generation technology.\\n\\nWhile no single metric can perfectly capture the nuanced qualities of text, MAUVE provides a valuable quantitative measure that can guide and inform evaluations of machine-generated content, helping researchers and developers in their efforts to improve text generation systems.\\n\\n<aside>\\nü§î Although you can assess the quality of machine generated text, this measure does not give you instance based scores. Which means, that you cannot consider a single piece of generated text and assess how good this particular instance is with respect to a human. More research might be needed on how to do this. Refer to my other post on how to do this for classification models.\\n\\n</aside>\\n\\nIf you would like to discuss the paper or exchange thoughts further, feel free to reach out via email at [**abkashyap92@protonmail.com**](mailto:abkashyap92@protonmail.com).\",\n",
       " '# Understanding Dataset Difficulty\\n\\nIn machine learning, many challenging questions often arise: how difficult is it for a model to classify a given example? how easy is the test dataset for a given model?, does a dataset for a given task like sentiment analysis harder than another dataset for a given model? These considerations are important in every day machine learning. \\n\\nEvery day, I grapple with the question: \"Have I crafted a dataset that truly challenges the capabilities of the model, or have I inadvertently made it too simplistic?\" Additionally, I continually ponder whether I have partitioned the dataset in a manner that presents a formidable challenge to the model\\'s learning process. In my pursuit of answers to these crucial questions, I read this paper that furnishes insightful solutions . \\n\\n<aside>\\nü§î ‚ÄúHave I crafted a dataset that truly challenges the capabilities of the model, or have I inadvertently made it too simplistic?‚Äù\\n\\n</aside>\\n\\nIn this paper, the authors employ information-theoretic measures‚Äîfear not, for the complexity of calculating this boils down to  training classification models‚Äîto address the questions surrounding dataset difficulty. They introduce a novel measure known as V-usable information to do this.\\n\\nLet\\'s consider an illustrative example: \"The food is delicious\" (a classic choice for various scenarios), which clearly conveys a positive sentiment. Now, juxtapose this with the text \"Uif gppe jt efmjdjpvt,\" which is actually an encryption of the initial sentence. The question that arises is: which of these two sentences provides more actionable information for a predictive AI model to discern the positive sentiment? Undoubtedly, it\\'s the first one.\\n\\nThis paper introduces a single metric ‚Äî the $\\\\mathcal{V}$-usable information ‚Äî that quantifies this notion of usable information. It is elegant and can be calculated for any classification problem as we will see next. \\n\\n## Formal definitions and Calculating Them\\n\\nLet\\'s consider a scenario where we have a dataset denoted as $X$, which is used for predicting classes $Y$. Additionally, we are working with a family of models represented as $\\\\mathcal{V}$ , such as the transformer family of models. $\\\\mathcal{V}$-usable information is losely based on mutual information .  It can be calculated using two steps. \\n\\n**Step 1:** Train a model on the dataset while deliberately providing it with empty input. Here, $\\\\empty$ signifies that the model receives no input ( \\'inf,\\' stands for infimum, representing the lowest value within a set).\\n\\n$$\\nH(Y) = inf_{f\\\\in V} E[-log_2 f[\\\\empty][Y]]\\n$$\\n\\n**Step 2:** Train a model on the training dataset, this time with actual input:\\n\\n$$\\nH(Y|X) = inf_{f\\\\in\\\\mathcal{V}}\\\\ E[-log_2\\\\ f[X][Y]]\\n$$\\n\\nThen calculate the $\\\\mathcal{V}$-usable information \\n\\n$$\\nI_{\\\\mathcal{V}} = H(Y) - H(Y|X)\\n$$\\n\\n<aside>\\nü§î This is reduction in entropy ‚Äî which is information. Imagine you are learning a concept and are confused about it. Now you read a blog post or someone helps you understand something that you are stuck on.  This helps you understand the concept in a better manner. Your confusion clears. You have gained some information.\\n\\n</aside>\\n\\nTo compute this measure, the authors provide a step-by-step algorithm in their paper. Essentially, it involves calculating the entropy (cross-entropy loss) for each instance in the test set and aggregating these values. Rest assured, this process is straightforward!‚Äù. \\n\\nThere is another measure that the authors introduce which allows you to calculate the hardness of the instance for a model ‚Äî the pointwise mutual information. Its calcualtion is similar to what is presented above. \\n\\n<aside>\\nü§î Train a classifier with input and train another classifier without input. That is it. You can calculate the $\\\\mathcal{V}$-usable information.\\n\\n</aside>\\n\\nHere are some things that you can do with this measure\\n\\n1. **Identify Challenging Datasets:** This measure can be employed to discern which datasets pose significant challenges for a particular family of models. For instance, consider the MNLI dataset, designed for Natural Language Inference (NLI), which encompasses various genres not found in the SNLI dataset. By evaluating whether MNLI is more challenging for the **`bert-base`** model, we can determine the extent to which certain data characteristics impact model performance.\\n2. **Detect Mislabelled Examples:** Low Pointwise $\\\\mathcal{V}$-Usable Information values may serve as an indicator of mislabelled instances within the test set. When this measure is low, it is advisable to scrutinize individual instances within the test dataset to identify and rectify labeling errors. This can lead to improvements in model performance and overall data quality.\\n\\n<aside>\\nü§î This paper only deals with a family of models that are suitable for classification only. How can we do siimlar kind of analysis for generative models still needs to be explored. For example, how hard is to for the model to translate this instance or to sumamrize this article that is more reelvant in the current times of large language models is still not known\\n\\n</aside>\\n\\nThe paper and the insights regarding the next frontier of innovation in dataset analysis and augmentation are intriguing. Analyzing and enhancing existing datasets are critical steps in improving the performance and robustness of large language models. This is especially important given the central role that these models play in various natural language processing tasks.\\n\\nIf you would like to discuss the paper or exchange thoughts further, feel free to reach out via email at [**abkashyap92@protonmail.com**](mailto:abkashyap92@protonmail.com).']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowme-Vm6gcGsG-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
